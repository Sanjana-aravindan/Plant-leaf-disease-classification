{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "#from skimage import transform\n",
    "import sys\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import copy\n",
    "from sklearn.metrics import f1_score\n",
    "import torchvision.models as models\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "from matplotlib import rcParams, animation, rc\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "from ipywidgets.widgets import *\n",
    "\n",
    "#from display import read_image, draw_boxes, draw_grid, draw_text\n",
    "#import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen GPU: 0\n"
     ]
    }
   ],
   "source": [
    "# with this function you set the value of the environment variable CUDA_VISIBLE_DEVICES\n",
    "# to set which GPU to use\n",
    "# it also reserves this amount of memory for your exclusive use. This might be important for \n",
    "# not having other people using the resources you need in shared systems\n",
    "# the homework was tested in a GPU with 4GB of memory, and running this function will require at least\n",
    "# as much\n",
    "# if you want to test in a GPU with less memory, you can call this function\n",
    "# with the argument minimum_memory_mb specifying how much memory from the GPU you want to reserve\n",
    "def define_gpu_to_use(minimum_memory_mb = 3800):\n",
    "    gpu_to_use = None\n",
    "    try: \n",
    "        os.environ['CUDA_VISIBLE_DEVICES']\n",
    "        print('GPU already assigned before: ' + str(os.environ['CUDA_VISIBLE_DEVICES']))\n",
    "        return\n",
    "    except:\n",
    "        pass\n",
    "    torch.cuda.empty_cache()\n",
    "    for i in range(16):\n",
    "        free_memory = !nvidia-smi --query-gpu=memory.free -i $i --format=csv,nounits,noheader\n",
    "        if free_memory[0] == 'No devices were found':\n",
    "            break\n",
    "        free_memory = int(free_memory[0])\n",
    "        if free_memory>minimum_memory_mb-500:\n",
    "            gpu_to_use = i\n",
    "            break\n",
    "    if gpu_to_use is None:\n",
    "        print('Could not find any GPU available with the required free memory of ' +str(minimum_memory_mb) + 'MB. Please use a different system for this assignment.')\n",
    "    else:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_to_use)\n",
    "        print('Chosen GPU: ' + str(gpu_to_use))\n",
    "        x = torch.rand((256,1024,minimum_memory_mb-500)).cuda()\n",
    "        x = torch.rand((1,1)).cuda()\n",
    "        del x\n",
    "define_gpu_to_use()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the train set:16051\n",
      "Length of the test set:6453\n",
      "Length of the validation set:4264\n",
      " \n",
      "The label names along with its corresponding key:{0: 'Apple_Frogeye_Spot', 1: 'Apple___Apple_scab', 2: 'Apple___Cedar_apple_rust', 3: 'Apple___healthy', 4: 'Blueberry___healthy', 5: 'Cherry_including_sour___Powdery_mildew', 6: 'Cherry_including_sour___healthy', 7: 'Corn_maize___Cercospora_leaf_spot Gray_leaf_spot', 8: 'Corn_maize___Common_rust_', 9: 'Corn_maize___Northern_Leaf_Blight', 10: 'Corn_maize___healthy', 11: 'Grape___Black_rot', 12: 'Grape___Esca_Black_Measles', 13: 'Grape___Leaf_blight_Isariopsis_Leaf_Spot', 14: 'Grape___healthy', 15: 'Orange___Haunglongbing_Citrus_greening', 16: 'Peach___Bacterial_spot', 17: 'Peach___healthy', 18: 'Pepper_bell___Bacterial_spot', 19: 'Pepper_bell___healthy', 20: 'Potato___Early_blight', 21: 'Potato___Late_blight', 22: 'Potato___healthy', 23: 'Raspberry___healthy', 24: 'Soybean___healthy', 25: 'Squash___Powdery_mildew', 26: 'Strawberry___Leaf_scorch', 27: 'Strawberry___healthy', 28: 'Tomato___Bacterial_spot', 29: 'Tomato___Early_blight', 30: 'Tomato___Late_blight', 31: 'Tomato___Leaf_Mold', 32: 'Tomato___Septoria_leaf_spot', 33: 'Tomato___Spider_mites_Two-spotted_spider_mite', 34: 'Tomato___Target_Spot', 35: 'Tomato___Tomato_Yellow_Leaf_Curl_Virus', 36: 'Tomato___Tomato_mosaic_virus', 37: 'Tomato___healthy'}\n"
     ]
    }
   ],
   "source": [
    "def load_raw_images(root,split):\n",
    "    set_of_transforms = {}\n",
    "    set_of_transforms['train'] = transforms.Compose(\n",
    "        [transforms.RandomRotation(30),\n",
    "         transforms.RandomResizedCrop(224),\n",
    "         transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])])\n",
    "    set_of_transforms['test'] = transforms.Compose(\n",
    "        [transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])])\n",
    "    set_of_transforms['validate'] = set_of_transforms['test']\n",
    "    \n",
    "\n",
    "    whole_dataset = torchvision.datasets.ImageFolder(root=root+'/'+split, transform=set_of_transforms[split])\n",
    "    \n",
    "    return whole_dataset   \n",
    "\n",
    "def load_grayscale_images(root,split):\n",
    "    set_of_transforms = {}\n",
    "    set_of_transforms['train'] = transforms.Compose(\n",
    "        [transforms.RandomRotation(30),\n",
    "         transforms.RandomResizedCrop(224),\n",
    "         transforms.RandomHorizontalFlip(),\n",
    "         transforms.Grayscale(num_output_channels=3),\n",
    "         transforms.ToTensor(), \n",
    "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])])\n",
    "    set_of_transforms['test'] = transforms.Compose(\n",
    "        [transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])])\n",
    "    set_of_transforms['validate'] = set_of_transforms['test']\n",
    "    \n",
    "\n",
    "    whole_dataset = torchvision.datasets.ImageFolder(root=root+'/'+split, transform=set_of_transforms[split])\n",
    "    \n",
    "    return whole_dataset   \n",
    "\n",
    "def find_classes(dir):\n",
    "    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
    "    classes.sort()\n",
    "    class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "    return classes, class_to_idx\n",
    "\n",
    "#color images\n",
    "path_to_img_folder = '../color/60_20_20'\n",
    "train_data = load_raw_images(path_to_img_folder,'train')\n",
    "test_data = load_raw_images(path_to_img_folder,'test')\n",
    "val_data = load_raw_images(path_to_img_folder,'validate')\n",
    "\n",
    "#grayscale images\n",
    "train_grayscale_data = load_grayscale_images(path_to_img_folder,'train')\n",
    "test_grayscale_data = load_grayscale_images(path_to_img_folder,'test')\n",
    "val_grayscale_data = load_grayscale_images(path_to_img_folder,'validate')\n",
    "\n",
    "#segmented images\n",
    "path_to_segment_img_folder = '../segment/60_20_20segmented'\n",
    "train_segmented_data = load_raw_images(path_to_segment_img_folder,'train')\n",
    "test_segmented_data = load_raw_images(path_to_segment_img_folder,'test')\n",
    "val_segmented_data = load_raw_images(path_to_segment_img_folder,'validate')\n",
    "\n",
    "\n",
    "label_names,label_indices = find_classes('../color/60_20_20/train')\n",
    "label_names_dict = dict((v,k) for k,v in label_indices.items())\n",
    "print (\"Length of the train set:{}\".format(len(train_data)))\n",
    "print (\"Length of the test set:{}\".format(len(test_data)))\n",
    "print (\"Length of the validation set:{}\".format(len(val_data)))\n",
    "print (\" \")\n",
    "print (\"The label names along with its corresponding key:{}\".format(label_names_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches train set:192\n",
      "Number of batches test set:77\n",
      "Number of batches validation set:51\n"
     ]
    }
   ],
   "source": [
    "#original images (RGB) data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_data, shuffle = True, batch_size = 84, num_workers = 3)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, shuffle = True, batch_size = 84, num_workers = 3)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, shuffle = True, batch_size = 84, num_workers = 3)\n",
    "\n",
    "#grayscale data loader\n",
    "train_grayscale_loader = torch.utils.data.DataLoader(train_grayscale_data, shuffle = True, batch_size = 84, num_workers = 3)\n",
    "test_grayscale_loader = torch.utils.data.DataLoader(test_grayscale_data, shuffle = True, batch_size = 84, num_workers = 3)\n",
    "val_grayscale_loader = torch.utils.data.DataLoader(val_grayscale_data, shuffle = True, batch_size = 84, num_workers = 3)\n",
    "\n",
    "#segmented images data loader\n",
    "train_segmented_loader = torch.utils.data.DataLoader(train_segmented_data, shuffle = True, batch_size = 84, num_workers = 3)\n",
    "test_segmented_loader = torch.utils.data.DataLoader(test_segmented_data, shuffle = True, batch_size = 84, num_workers = 3)\n",
    "val_segmented_loader = torch.utils.data.DataLoader(val_segmented_data, shuffle = True, batch_size = 84, num_workers = 3)\n",
    "\n",
    "#visualization\n",
    "train_loader_vis = torch.utils.data.DataLoader(train_data, shuffle = True, batch_size = 1, num_workers = 3)\n",
    "\n",
    "print (\"Number of batches train set:{}\".format(len(train_loader)))\n",
    "print (\"Number of batches test set:{}\".format(len(test_loader)))\n",
    "print (\"Number of batches validation set:{}\".format(len(val_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet(\n",
      "  (features): Sequential(\n",
      "    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu0): ReLU(inplace)\n",
      "    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (denseblock1): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition1): _Transition(\n",
      "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock2): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition2): _Transition(\n",
      "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock3): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer13): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer14): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer15): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer16): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer17): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer18): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer19): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer20): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer21): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer22): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer23): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer24): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer25): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer26): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(1056, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer27): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1088, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(1088, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer28): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(1120, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer29): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(1152, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer30): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(1184, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer31): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(1216, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer32): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(1248, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition3): _Transition(\n",
      "      (norm): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock4): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer13): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer14): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(1056, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer15): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1088, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(1088, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer16): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(1120, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer17): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(1152, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer18): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(1184, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer19): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(1216, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer20): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(1248, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer21): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(1280, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer22): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1312, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(1312, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer23): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(1344, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer24): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1376, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(1376, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer25): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1408, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(1408, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer26): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1440, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(1440, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer27): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1472, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(1472, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer28): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1504, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(1504, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer29): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(1536, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer30): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1568, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(1568, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer31): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(1600, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer32): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace)\n",
      "        (conv1): Conv2d(1632, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (norm5): BatchNorm2d(1664, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=1664, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#densenet169\n",
    "classification_model = models.densenet169(pretrained=True)\n",
    "for param in classification_model.parameters():\n",
    "    param.requires_grad = False\n",
    "print (classification_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier=torch.nn.Sequential(OrderedDict([('fc1', torch.nn.Linear(1664, 512)),\n",
    "                           ('relu', torch.nn.ReLU()), \n",
    "                           ('dropout', torch.nn.Dropout(p=0.337)),\n",
    "                           ('fc2', torch.nn.Linear(512, 38)),\n",
    "                           ('output', torch.nn.LogSoftmax(dim=1))\n",
    "                             ]))\n",
    "classification_model.classifier = classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classification_model(data_loader, model, criterion, optimizer, lr_decay, num_epochs,mode='train'):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        loss_sum = []; acc = 0\n",
    "        for i in data_loader:\n",
    "            input_img =i[0].cuda()\n",
    "            target = i[1].cuda() \n",
    "            if mode=='train':\n",
    "                optimizer.zero_grad()\n",
    "            predicted_output = model(input_img)\n",
    "            loss = criterion(predicted_output, target)\n",
    "            _,pred_out = torch.max(predicted_output,1)    \n",
    "            if mode=='train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            loss_sum.append(loss.item())\n",
    "            acc+=torch.sum(pred_out==target.data)\n",
    "        epoch_loss = sum(loss_sum)/len(loss_sum)\n",
    "        epoch_accuracy = acc.item()/len(train_data)\n",
    "        print (\"epoch:\"+str(epoch))\n",
    "        print (\"Loss:{:.6f}\".format(epoch_loss))\n",
    "        print (\"Accuracy:{:.6f}\".format(epoch_accuracy))\n",
    "        print (\" \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test models\n",
    "def test_model(model,data_loader):\n",
    "    model.eval()\n",
    "    accuracy_list=[]\n",
    "    with torch.no_grad():\n",
    "        predicted_output_concat = np.zeros([84])\n",
    "        target_concat = np.zeros([84])\n",
    "        for i in data_loader:\n",
    "            input_img=i[0].cuda()\n",
    "            #print (input_img[0].shape)\n",
    "            target=i[1]\n",
    "            target=target.cuda()\n",
    "            pred_output = model(input_img)\n",
    "            eq = (target.data == torch.exp(pred_output).max(1)[1])\n",
    "            pred_out = torch.exp(pred_output).max(1)[1]\n",
    "            accuracy_list.append(eq.type_as(torch.FloatTensor()).mean())\n",
    "            predicted_output_concat = np.concatenate((predicted_output_concat, pred_out.cpu().detach().numpy()), axis = 0)\n",
    "            target_concat = np.concatenate((target_concat, target.cpu().detach().numpy()), axis = 0)\n",
    "        print (\"Test accuracy:{:.6f}\".format(sum(accuracy_list)/len(accuracy_list)))\n",
    "        #print (target_concat)\n",
    "        #print (target_concat.shape)\n",
    "        #print (predicted_output_concat)\n",
    "        #print (predicted_output_concat.shape)\n",
    "    print (\"Macro F1 score:{}\".format(f1_score(target_concat, predicted_output_concat, average=\"macro\")))\n",
    "    print (\"Micro F1 score:{}\".format(f1_score(target_concat, predicted_output_concat, average=\"micro\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam + 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet accuracies on color datatset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "Loss:1.370612\n",
      "Accuracy:0.641268\n",
      " \n",
      "epoch:1\n",
      "Loss:0.623804\n",
      "Accuracy:0.816772\n",
      " \n",
      "epoch:2\n",
      "Loss:0.511978\n",
      "Accuracy:0.847922\n",
      " \n",
      "epoch:3\n",
      "Loss:0.449003\n",
      "Accuracy:0.863560\n",
      " \n",
      "epoch:4\n",
      "Loss:0.417524\n",
      "Accuracy:0.870600\n",
      " \n",
      "epoch:5\n",
      "Loss:0.402888\n",
      "Accuracy:0.875023\n",
      " \n",
      "epoch:6\n",
      "Loss:0.398371\n",
      "Accuracy:0.876207\n",
      " \n",
      "epoch:7\n",
      "Loss:0.377754\n",
      "Accuracy:0.881627\n",
      " \n",
      "epoch:8\n",
      "Loss:0.375874\n",
      "Accuracy:0.884306\n",
      " \n",
      "epoch:9\n",
      "Loss:0.351190\n",
      "Accuracy:0.891533\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "criterion = torch.nn.NLLLoss()\n",
    "classification_model = classification_model.cuda()\n",
    "optimizer = torch.optim.Adam(classification_model.classifier.parameters(), lr=0.001)\n",
    "#lr_schedule = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "lr_schedule = 0 \n",
    "train_classification_model(train_loader, classification_model, criterion, optimizer,lr_schedule,10,'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.908132\n",
      "[ 0.  0.  0. ... 21.  5. 28.]\n",
      "(4348,)\n",
      "[ 0.  0.  0. ... 21.  5. 28.]\n",
      "(4348,)\n",
      "Macro F1 score:0.8734251246469528\n",
      "Micro F1 score:0.9098436062557498\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.918509\n",
      "[ 0.  0.  0. ... 15. 34. 10.]\n",
      "(6537,)\n",
      "[ 0.  0.  0. ... 15. 34. 10.]\n",
      "(6537,)\n",
      "Macro F1 score:0.8851078312353295\n",
      "Micro F1 score:0.9195349548722656\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet accuracies on segmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "Loss:1.378010\n",
      "Accuracy:0.630615\n",
      " \n",
      "epoch:1\n",
      "Loss:0.678285\n",
      "Accuracy:0.796025\n",
      " \n",
      "epoch:2\n",
      "Loss:0.566608\n",
      "Accuracy:0.826553\n",
      " \n",
      "epoch:3\n",
      "Loss:0.512933\n",
      "Accuracy:0.838577\n",
      " \n",
      "epoch:4\n",
      "Loss:0.486442\n",
      "Accuracy:0.844994\n",
      " \n",
      "epoch:5\n",
      "Loss:0.450192\n",
      "Accuracy:0.858015\n",
      " \n",
      "epoch:6\n",
      "Loss:0.466206\n",
      "Accuracy:0.852408\n",
      " \n",
      "epoch:7\n",
      "Loss:0.437082\n",
      "Accuracy:0.861815\n",
      " \n",
      "epoch:8\n",
      "Loss:0.420607\n",
      "Accuracy:0.864245\n",
      " \n",
      "epoch:9\n",
      "Loss:0.422610\n",
      "Accuracy:0.867796\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "criterion = torch.nn.NLLLoss()\n",
    "classification_model = classification_model.cuda()\n",
    "optimizer = torch.optim.Adam(classification_model.classifier.parameters(), lr=0.001)\n",
    "#lr_schedule = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "lr_schedule = 0 \n",
    "train_classification_model(train_segmented_loader, classification_model, criterion, optimizer,lr_schedule,10,'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.903783\n",
      "[ 0.  0.  0. ... 35. 28. 24.]\n",
      "(4346,)\n",
      "[ 0.  0.  0. ... 35. 28. 24.]\n",
      "(4346,)\n",
      "Macro F1 score:0.8683163237904613\n",
      "Micro F1 score:0.9058904739990796\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, val_segmented_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.907669\n",
      "[ 0.  0.  0. ... 35. 12.  8.]\n",
      "(6530,)\n",
      "[ 0.  0.  0. ... 35. 12.  8.]\n",
      "(6530,)\n",
      "Macro F1 score:0.8778762779203025\n",
      "Micro F1 score:0.9090352220520673\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, test_segmented_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet accuracies on grayscale dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "Loss:1.654747\n",
      "Accuracy:0.554919\n",
      " \n",
      "epoch:1\n",
      "Loss:0.979705\n",
      "Accuracy:0.707121\n",
      " \n",
      "epoch:2\n",
      "Loss:0.840886\n",
      "Accuracy:0.742321\n",
      " \n",
      "epoch:3\n",
      "Loss:0.786080\n",
      "Accuracy:0.756090\n",
      " \n",
      "epoch:4\n",
      "Loss:0.739952\n",
      "Accuracy:0.769547\n",
      " \n",
      "epoch:5\n",
      "Loss:0.705836\n",
      "Accuracy:0.779951\n",
      " \n",
      "epoch:6\n",
      "Loss:0.690023\n",
      "Accuracy:0.783814\n",
      " \n",
      "epoch:7\n",
      "Loss:0.685005\n",
      "Accuracy:0.788611\n",
      " \n",
      "epoch:8\n",
      "Loss:0.691969\n",
      "Accuracy:0.783689\n",
      " \n",
      "epoch:9\n",
      "Loss:0.670668\n",
      "Accuracy:0.793159\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "criterion = torch.nn.NLLLoss()\n",
    "classification_model = classification_model.cuda()\n",
    "optimizer = torch.optim.Adam(classification_model.classifier.parameters(), lr=0.001)\n",
    "#lr_schedule = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "lr_schedule = 0 \n",
    "train_classification_model(train_grayscale_loader, classification_model, criterion, optimizer,lr_schedule,10,'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.813944\n",
      "[ 0.  0.  0. ... 18. 32. 24.]\n",
      "(4348,)\n",
      "[ 0.  0.  0. ... 18. 32. 27.]\n",
      "(4348,)\n",
      "Macro F1 score:0.7345969795978357\n",
      "Micro F1 score:0.8176172953081877\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, val_grayscale_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.830026\n",
      "[ 0.  0.  0. ... 37. 37. 24.]\n",
      "(6537,)\n",
      "[ 0.  0.  0. ... 37. 37. 24.]\n",
      "(6537,)\n",
      "Macro F1 score:0.7540847679787847\n",
      "Micro F1 score:0.8321860180510938\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, test_grayscale_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam + 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet accuracies on colored dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "Loss:2.633979\n",
      "Accuracy:0.367454\n",
      " \n",
      "epoch:1\n",
      "Loss:1.579117\n",
      "Accuracy:0.641082\n",
      " \n",
      "epoch:2\n",
      "Loss:1.135039\n",
      "Accuracy:0.729986\n",
      " \n",
      "epoch:3\n",
      "Loss:0.906096\n",
      "Accuracy:0.781758\n",
      " \n",
      "epoch:4\n",
      "Loss:0.787531\n",
      "Accuracy:0.799078\n",
      " \n",
      "epoch:5\n",
      "Loss:0.692816\n",
      "Accuracy:0.822067\n",
      " \n",
      "epoch:6\n",
      "Loss:0.632378\n",
      "Accuracy:0.833406\n",
      " \n",
      "epoch:7\n",
      "Loss:0.582675\n",
      "Accuracy:0.845306\n",
      " \n",
      "epoch:8\n",
      "Loss:0.531078\n",
      "Accuracy:0.857143\n",
      " \n",
      "epoch:9\n",
      "Loss:0.515038\n",
      "Accuracy:0.857641\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "criterion = torch.nn.NLLLoss()\n",
    "classification_model = classification_model.cuda()\n",
    "optimizer = torch.optim.Adam(classification_model.classifier.parameters(), lr=0.0001)\n",
    "#lr_schedule = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "lr_schedule = 0 \n",
    "train_classification_model(train_loader, classification_model, criterion, optimizer,lr_schedule,10,'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.915470\n",
      "Macro F1 score:0.8716233409760225\n",
      "Micro F1 score:0.9174333026678932\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.922562\n",
      "Macro F1 score:0.8811526792147862\n",
      "Micro F1 score:0.9235123145173627\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet accuracies on segmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "Loss:2.604974\n",
      "Accuracy:0.359479\n",
      " \n",
      "epoch:1\n",
      "Loss:1.579855\n",
      "Accuracy:0.621706\n",
      " \n",
      "epoch:2\n",
      "Loss:1.158403\n",
      "Accuracy:0.713663\n",
      " \n",
      "epoch:3\n",
      "Loss:0.944101\n",
      "Accuracy:0.756464\n",
      " \n",
      "epoch:4\n",
      "Loss:0.823808\n",
      "Accuracy:0.777272\n",
      " \n",
      "epoch:5\n",
      "Loss:0.731480\n",
      "Accuracy:0.802068\n",
      " \n",
      "epoch:6\n",
      "Loss:0.680121\n",
      "Accuracy:0.814217\n",
      " \n",
      "epoch:7\n",
      "Loss:0.627965\n",
      "Accuracy:0.825681\n",
      " \n",
      "epoch:8\n",
      "Loss:0.585611\n",
      "Accuracy:0.836272\n",
      " \n",
      "epoch:9\n",
      "Loss:0.567634\n",
      "Accuracy:0.838826\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "criterion = torch.nn.NLLLoss()\n",
    "classification_model = classification_model.cuda()\n",
    "optimizer = torch.optim.Adam(classification_model.classifier.parameters(), lr=0.0001)\n",
    "#lr_schedule = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "lr_schedule = 0 \n",
    "train_classification_model(train_segmented_loader, classification_model, criterion, optimizer,lr_schedule,10,'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.887225\n",
      "Macro F1 score:0.8344310634582476\n",
      "Micro F1 score:0.8893235158766684\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, val_segmented_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.890717\n",
      "Macro F1 score:0.8562902324180538\n",
      "Micro F1 score:0.8921898928024502\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, test_segmented_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet accuracies on grayscale dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "Loss:2.764966\n",
      "Accuracy:0.321101\n",
      " \n",
      "epoch:1\n",
      "Loss:1.869660\n",
      "Accuracy:0.542334\n",
      " \n",
      "epoch:2\n",
      "Loss:1.458904\n",
      "Accuracy:0.623575\n",
      " \n",
      "epoch:3\n",
      "Loss:1.237538\n",
      "Accuracy:0.677777\n",
      " \n",
      "epoch:4\n",
      "Loss:1.113610\n",
      "Accuracy:0.699209\n",
      " \n",
      "epoch:5\n",
      "Loss:1.028021\n",
      "Accuracy:0.715532\n",
      " \n",
      "epoch:6\n",
      "Loss:0.953009\n",
      "Accuracy:0.732602\n",
      " \n",
      "epoch:7\n",
      "Loss:0.906718\n",
      "Accuracy:0.744315\n",
      " \n",
      "epoch:8\n",
      "Loss:0.870253\n",
      "Accuracy:0.749112\n",
      " \n",
      "epoch:9\n",
      "Loss:0.824943\n",
      "Accuracy:0.762071\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "criterion = torch.nn.NLLLoss()\n",
    "classification_model = classification_model.cuda()\n",
    "optimizer = torch.optim.Adam(classification_model.classifier.parameters(), lr=0.0001)\n",
    "#lr_schedule = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "lr_schedule = 0 \n",
    "train_classification_model(train_grayscale_loader, classification_model, criterion, optimizer,lr_schedule,10,'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.799384\n",
      "Macro F1 score:0.7157554019447123\n",
      "Micro F1 score:0.8031278748850046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u1136693/env_dir/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, val_grayscale_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.808227\n",
      "Macro F1 score:0.7313127316161322\n",
      "Micro F1 score:0.8106164907449901\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, test_grayscale_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD + Momentum and 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet Accuracies on colored dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "Loss:3.103117\n",
      "Accuracy:0.234378\n",
      " \n",
      "epoch:1\n",
      "Loss:2.361066\n",
      "Accuracy:0.422030\n",
      " \n",
      "epoch:2\n",
      "Loss:1.839527\n",
      "Accuracy:0.574357\n",
      " \n",
      "epoch:3\n",
      "Loss:1.500082\n",
      "Accuracy:0.646128\n",
      " \n",
      "epoch:4\n",
      "Loss:1.270327\n",
      "Accuracy:0.692729\n",
      " \n",
      "epoch:5\n",
      "Loss:1.111358\n",
      "Accuracy:0.725811\n",
      " \n",
      "epoch:6\n",
      "Loss:1.002574\n",
      "Accuracy:0.750109\n",
      " \n",
      "epoch:7\n",
      "Loss:0.912586\n",
      "Accuracy:0.766058\n",
      " \n",
      "epoch:8\n",
      "Loss:0.843601\n",
      "Accuracy:0.782568\n",
      " \n",
      "epoch:9\n",
      "Loss:0.783495\n",
      "Accuracy:0.797957\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "criterion = torch.nn.NLLLoss()\n",
    "classification_model = classification_model.cuda()\n",
    "optimizer = torch.optim.SGD(classification_model.classifier.parameters(), lr=0.001, momentum=0.9)\n",
    "#lr_schedule = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "lr_schedule = 0 \n",
    "train_classification_model(train_loader, classification_model, criterion, optimizer,lr_schedule,10,'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.869325\n",
      "Macro F1 score:0.7657349925026674\n",
      "Micro F1 score:0.8718951241950322\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.879158\n",
      "Macro F1 score:0.779430599809091\n",
      "Micro F1 score:0.8808321860180511\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet Accuracies on segemented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "Loss:3.077225\n",
      "Accuracy:0.234378\n",
      " \n",
      "epoch:1\n",
      "Loss:2.341298\n",
      "Accuracy:0.425020\n",
      " \n",
      "epoch:2\n",
      "Loss:1.832699\n",
      "Accuracy:0.568314\n",
      " \n",
      "epoch:3\n",
      "Loss:1.514857\n",
      "Accuracy:0.627562\n",
      " \n",
      "epoch:4\n",
      "Loss:1.294201\n",
      "Accuracy:0.673914\n",
      " \n",
      "epoch:5\n",
      "Loss:1.139066\n",
      "Accuracy:0.710610\n",
      " \n",
      "epoch:6\n",
      "Loss:1.032650\n",
      "Accuracy:0.726746\n",
      " \n",
      "epoch:7\n",
      "Loss:0.945635\n",
      "Accuracy:0.750234\n",
      " \n",
      "epoch:8\n",
      "Loss:0.886963\n",
      "Accuracy:0.762071\n",
      " \n",
      "epoch:9\n",
      "Loss:0.829973\n",
      "Accuracy:0.775528\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "criterion = torch.nn.NLLLoss()\n",
    "classification_model = classification_model.cuda()\n",
    "optimizer = torch.optim.SGD(classification_model.classifier.parameters(), lr=0.001, momentum=0.9)\n",
    "#lr_schedule = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "lr_schedule = 0 \n",
    "train_classification_model(train_segmented_loader, classification_model, criterion, optimizer,lr_schedule,10,'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.849944\n",
      "Macro F1 score:0.75830165326387\n",
      "Micro F1 score:0.8527381500230097\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, val_segmented_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.848799\n",
      "Macro F1 score:0.7628943711002876\n",
      "Micro F1 score:0.8509954058192956\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, test_segmented_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet Accuracies on grayscale dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "Loss:3.126634\n",
      "Accuracy:0.231761\n",
      " \n",
      "epoch:1\n",
      "Loss:2.499142\n",
      "Accuracy:0.381409\n",
      " \n",
      "epoch:2\n",
      "Loss:2.073859\n",
      "Accuracy:0.493676\n",
      " \n",
      "epoch:3\n",
      "Loss:1.771320\n",
      "Accuracy:0.559591\n",
      " \n",
      "epoch:4\n",
      "Loss:1.577489\n",
      "Accuracy:0.595851\n",
      " \n",
      "epoch:5\n",
      "Loss:1.410192\n",
      "Accuracy:0.628247\n",
      " \n",
      "epoch:6\n",
      "Loss:1.307244\n",
      "Accuracy:0.651548\n",
      " \n",
      "epoch:7\n",
      "Loss:1.218548\n",
      "Accuracy:0.670488\n",
      " \n",
      "epoch:8\n",
      "Loss:1.153140\n",
      "Accuracy:0.677715\n",
      " \n",
      "epoch:9\n",
      "Loss:1.097252\n",
      "Accuracy:0.696156\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "criterion = torch.nn.NLLLoss()\n",
    "classification_model = classification_model.cuda()\n",
    "optimizer = torch.optim.SGD(classification_model.classifier.parameters(), lr=0.001, momentum=0.9)\n",
    "#lr_schedule = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "lr_schedule = 0 \n",
    "train_classification_model(train_grayscale_loader, classification_model, criterion, optimizer,lr_schedule,10,'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.751036\n",
      "Macro F1 score:0.6233183181398452\n",
      "Micro F1 score:0.7562097516099356\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, val_grayscale_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.762402\n",
      "Macro F1 score:0.6433805887978354\n",
      "Micro F1 score:0.7656417316811993\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, test_grayscale_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD + Momentum and 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet Accuracies on colored dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "Loss:3.537405\n",
      "Accuracy:0.108965\n",
      " \n",
      "epoch:1\n",
      "Loss:3.337838\n",
      "Accuracy:0.197246\n",
      " \n",
      "epoch:2\n",
      "Loss:3.223786\n",
      "Accuracy:0.233817\n",
      " \n",
      "epoch:3\n",
      "Loss:3.136199\n",
      "Accuracy:0.252445\n",
      " \n",
      "epoch:4\n",
      "Loss:3.058843\n",
      "Accuracy:0.265529\n",
      " \n",
      "epoch:5\n",
      "Loss:2.972534\n",
      "Accuracy:0.274625\n",
      " \n",
      "epoch:6\n",
      "Loss:2.896929\n",
      "Accuracy:0.287521\n",
      " \n",
      "epoch:7\n",
      "Loss:2.808199\n",
      "Accuracy:0.302100\n",
      " \n",
      "epoch:8\n",
      "Loss:2.727082\n",
      "Accuracy:0.321787\n",
      " \n",
      "epoch:9\n",
      "Loss:2.650814\n",
      "Accuracy:0.343032\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "criterion = torch.nn.NLLLoss()\n",
    "classification_model = classification_model.cuda()\n",
    "optimizer = torch.optim.SGD(classification_model.classifier.parameters(), lr=0.0001, momentum=0.9)\n",
    "#lr_schedule = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "lr_schedule = 0 \n",
    "train_classification_model(train_loader, classification_model, criterion, optimizer,lr_schedule,10,'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.371659\n",
      "Macro F1 score:0.1414947772089892\n",
      "Micro F1 score:0.38385464581416745\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.380811\n",
      "Macro F1 score:0.14379367822766265\n",
      "Micro F1 score:0.38871041762276276\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet Accuracies on segmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "Loss:3.539915\n",
      "Accuracy:0.104479\n",
      " \n",
      "epoch:1\n",
      "Loss:3.331700\n",
      "Accuracy:0.193633\n",
      " \n",
      "epoch:2\n",
      "Loss:3.218904\n",
      "Accuracy:0.227151\n",
      " \n",
      "epoch:3\n",
      "Loss:3.135442\n",
      "Accuracy:0.251511\n",
      " \n",
      "epoch:4\n",
      "Loss:3.056415\n",
      "Accuracy:0.265342\n",
      " \n",
      "epoch:5\n",
      "Loss:2.975466\n",
      "Accuracy:0.272693\n",
      " \n",
      "epoch:6\n",
      "Loss:2.893177\n",
      "Accuracy:0.284780\n",
      " \n",
      "epoch:7\n",
      "Loss:2.813249\n",
      "Accuracy:0.294935\n",
      " \n",
      "epoch:8\n",
      "Loss:2.735249\n",
      "Accuracy:0.306523\n",
      " \n",
      "epoch:9\n",
      "Loss:2.656679\n",
      "Accuracy:0.331319\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "criterion = torch.nn.NLLLoss()\n",
    "classification_model = classification_model.cuda()\n",
    "optimizer = torch.optim.SGD(classification_model.classifier.parameters(), lr=0.0001, momentum=0.9)\n",
    "#lr_schedule = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "lr_schedule = 0 \n",
    "train_classification_model(train_segmented_loader, classification_model, criterion, optimizer,lr_schedule,10,'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.336157\n",
      "Macro F1 score:0.10078145675938845\n",
      "Micro F1 score:0.3490566037735849\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, val_segmented_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.343353\n",
      "Macro F1 score:0.10025174688745901\n",
      "Micro F1 score:0.35176110260336907\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, test_segmented_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet Accuracies on grayscale dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "Loss:3.524429\n",
      "Accuracy:0.092705\n",
      " \n",
      "epoch:1\n",
      "Loss:3.341487\n",
      "Accuracy:0.150707\n",
      " \n",
      "epoch:2\n",
      "Loss:3.256544\n",
      "Accuracy:0.201981\n",
      " \n",
      "epoch:3\n",
      "Loss:3.183970\n",
      "Accuracy:0.233506\n",
      " \n",
      "epoch:4\n",
      "Loss:3.122438\n",
      "Accuracy:0.250950\n",
      " \n",
      "epoch:5\n",
      "Loss:3.058310\n",
      "Accuracy:0.261791\n",
      " \n",
      "epoch:6\n",
      "Loss:2.992608\n",
      "Accuracy:0.269827\n",
      " \n",
      "epoch:7\n",
      "Loss:2.926996\n",
      "Accuracy:0.277677\n",
      " \n",
      "epoch:8\n",
      "Loss:2.855185\n",
      "Accuracy:0.287521\n",
      " \n",
      "epoch:9\n",
      "Loss:2.794666\n",
      "Accuracy:0.300978\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "criterion = torch.nn.NLLLoss()\n",
    "classification_model = classification_model.cuda()\n",
    "optimizer = torch.optim.SGD(classification_model.classifier.parameters(), lr=0.0001, momentum=0.9)\n",
    "#lr_schedule = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "lr_schedule = 0 \n",
    "train_classification_model(train_grayscale_loader, classification_model, criterion, optimizer,lr_schedule,10,'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.324113\n",
      "Macro F1 score:0.11529917761094383\n",
      "Micro F1 score:0.33693652253909845\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, val_grayscale_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.329677\n",
      "Macro F1 score:0.11409441986511411\n",
      "Micro F1 score:0.3383815205751874\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, test_grayscale_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dir",
   "language": "python",
   "name": "env_dir"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
