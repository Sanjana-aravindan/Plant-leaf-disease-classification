{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "#from skimage import transform\n",
    "import sys\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import copy\n",
    "from sklearn.metrics import f1_score\n",
    "import torchvision.models as models\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "from matplotlib import rcParams, animation, rc\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "from ipywidgets.widgets import *\n",
    "\n",
    "#from display import read_image, draw_boxes, draw_grid, draw_text\n",
    "#import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen GPU: 0\n"
     ]
    }
   ],
   "source": [
    "# with this function you set the value of the environment variable CUDA_VISIBLE_DEVICES\n",
    "# to set which GPU to use\n",
    "# it also reserves this amount of memory for your exclusive use. This might be important for \n",
    "# not having other people using the resources you need in shared systems\n",
    "# the homework was tested in a GPU with 4GB of memory, and running this function will require at least\n",
    "# as much\n",
    "# if you want to test in a GPU with less memory, you can call this function\n",
    "# with the argument minimum_memory_mb specifying how much memory from the GPU you want to reserve\n",
    "def define_gpu_to_use(minimum_memory_mb = 3800):\n",
    "    gpu_to_use = None\n",
    "    try: \n",
    "        os.environ['CUDA_VISIBLE_DEVICES']\n",
    "        print('GPU already assigned before: ' + str(os.environ['CUDA_VISIBLE_DEVICES']))\n",
    "        return\n",
    "    except:\n",
    "        pass\n",
    "    torch.cuda.empty_cache()\n",
    "    for i in range(16):\n",
    "        free_memory = !nvidia-smi --query-gpu=memory.free -i $i --format=csv,nounits,noheader\n",
    "        if free_memory[0] == 'No devices were found':\n",
    "            break\n",
    "        free_memory = int(free_memory[0])\n",
    "        if free_memory>minimum_memory_mb-500:\n",
    "            gpu_to_use = i\n",
    "            break\n",
    "    if gpu_to_use is None:\n",
    "        print('Could not find any GPU available with the required free memory of ' +str(minimum_memory_mb) + 'MB. Please use a different system for this assignment.')\n",
    "    else:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_to_use)\n",
    "        print('Chosen GPU: ' + str(gpu_to_use))\n",
    "        x = torch.rand((256,1024,minimum_memory_mb-500)).cuda()\n",
    "        x = torch.rand((1,1)).cuda()\n",
    "        del x\n",
    "define_gpu_to_use()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the train set:16051\n",
      "Length of the test set:6453\n",
      "Length of the validation set:4264\n",
      " \n",
      "The label names along with its corresponding key:{0: 'Apple_Frogeye_Spot', 1: 'Apple___Apple_scab', 2: 'Apple___Cedar_apple_rust', 3: 'Apple___healthy', 4: 'Blueberry___healthy', 5: 'Cherry_including_sour___Powdery_mildew', 6: 'Cherry_including_sour___healthy', 7: 'Corn_maize___Cercospora_leaf_spot Gray_leaf_spot', 8: 'Corn_maize___Common_rust_', 9: 'Corn_maize___Northern_Leaf_Blight', 10: 'Corn_maize___healthy', 11: 'Grape___Black_rot', 12: 'Grape___Esca_Black_Measles', 13: 'Grape___Leaf_blight_Isariopsis_Leaf_Spot', 14: 'Grape___healthy', 15: 'Orange___Haunglongbing_Citrus_greening', 16: 'Peach___Bacterial_spot', 17: 'Peach___healthy', 18: 'Pepper_bell___Bacterial_spot', 19: 'Pepper_bell___healthy', 20: 'Potato___Early_blight', 21: 'Potato___Late_blight', 22: 'Potato___healthy', 23: 'Raspberry___healthy', 24: 'Soybean___healthy', 25: 'Squash___Powdery_mildew', 26: 'Strawberry___Leaf_scorch', 27: 'Strawberry___healthy', 28: 'Tomato___Bacterial_spot', 29: 'Tomato___Early_blight', 30: 'Tomato___Late_blight', 31: 'Tomato___Leaf_Mold', 32: 'Tomato___Septoria_leaf_spot', 33: 'Tomato___Spider_mites_Two-spotted_spider_mite', 34: 'Tomato___Target_Spot', 35: 'Tomato___Tomato_Yellow_Leaf_Curl_Virus', 36: 'Tomato___Tomato_mosaic_virus', 37: 'Tomato___healthy'}\n"
     ]
    }
   ],
   "source": [
    "def load_raw_images(root,split):\n",
    "    set_of_transforms = {}\n",
    "    set_of_transforms['train'] = transforms.Compose(\n",
    "        [transforms.RandomRotation(30),\n",
    "         transforms.RandomResizedCrop(224),\n",
    "         transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])])\n",
    "    set_of_transforms['test'] = transforms.Compose(\n",
    "        [transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])])\n",
    "    set_of_transforms['validate'] = set_of_transforms['test']\n",
    "    \n",
    "\n",
    "    whole_dataset = torchvision.datasets.ImageFolder(root=root+'/'+split, transform=set_of_transforms[split])\n",
    "    \n",
    "    return whole_dataset   \n",
    "\n",
    "def load_grayscale_images(root,split):\n",
    "    set_of_transforms = {}\n",
    "    set_of_transforms['train'] = transforms.Compose(\n",
    "        [transforms.RandomRotation(30),\n",
    "         transforms.RandomResizedCrop(224),\n",
    "         transforms.RandomHorizontalFlip(),\n",
    "         transforms.Grayscale(num_output_channels=3),\n",
    "         transforms.ToTensor(), \n",
    "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])])\n",
    "    set_of_transforms['test'] = transforms.Compose(\n",
    "        [transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])])\n",
    "    set_of_transforms['validate'] = set_of_transforms['test']\n",
    "    \n",
    "\n",
    "    whole_dataset = torchvision.datasets.ImageFolder(root=root+'/'+split, transform=set_of_transforms[split])\n",
    "    \n",
    "    return whole_dataset   \n",
    "\n",
    "def find_classes(dir):\n",
    "    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
    "    classes.sort()\n",
    "    class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "    return classes, class_to_idx\n",
    "\n",
    "#color images\n",
    "path_to_img_folder = '../color/60_20_20'\n",
    "train_data = load_raw_images(path_to_img_folder,'train')\n",
    "test_data = load_raw_images(path_to_img_folder,'test')\n",
    "val_data = load_raw_images(path_to_img_folder,'validate')\n",
    "\n",
    "#grayscale images\n",
    "train_grayscale_data = load_grayscale_images(path_to_img_folder,'train')\n",
    "test_grayscale_data = load_grayscale_images(path_to_img_folder,'test')\n",
    "val_grayscale_data = load_grayscale_images(path_to_img_folder,'validate')\n",
    "\n",
    "#segmented images\n",
    "path_to_segment_img_folder = '../segment/60_20_20segmented'\n",
    "train_segmented_data = load_raw_images(path_to_segment_img_folder,'train')\n",
    "test_segmented_data = load_raw_images(path_to_segment_img_folder,'test')\n",
    "val_segmented_data = load_raw_images(path_to_segment_img_folder,'validate')\n",
    "\n",
    "\n",
    "label_names,label_indices = find_classes('../color/60_20_20/train')\n",
    "label_names_dict = dict((v,k) for k,v in label_indices.items())\n",
    "print (\"Length of the train set:{}\".format(len(train_data)))\n",
    "print (\"Length of the test set:{}\".format(len(test_data)))\n",
    "print (\"Length of the validation set:{}\".format(len(val_data)))\n",
    "print (\" \")\n",
    "print (\"The label names along with its corresponding key:{}\".format(label_names_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches train set:192\n",
      "Number of batches test set:77\n",
      "Number of batches validation set:51\n"
     ]
    }
   ],
   "source": [
    "#original images (RGB) data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_data, shuffle = True, batch_size = 84, num_workers = 3)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, shuffle = True, batch_size = 84, num_workers = 3)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, shuffle = True, batch_size = 84, num_workers = 3)\n",
    "\n",
    "#grayscale data loader\n",
    "train_grayscale_loader = torch.utils.data.DataLoader(train_grayscale_data, shuffle = True, batch_size = 84, num_workers = 3)\n",
    "test_grayscale_loader = torch.utils.data.DataLoader(test_grayscale_data, shuffle = True, batch_size = 84, num_workers = 3)\n",
    "val_grayscale_loader = torch.utils.data.DataLoader(val_grayscale_data, shuffle = True, batch_size = 84, num_workers = 3)\n",
    "\n",
    "#segmented images data loader\n",
    "train_segmented_loader = torch.utils.data.DataLoader(train_segmented_data, shuffle = True, batch_size = 84, num_workers = 3)\n",
    "test_segmented_loader = torch.utils.data.DataLoader(test_segmented_data, shuffle = True, batch_size = 84, num_workers = 3)\n",
    "val_segmented_loader = torch.utils.data.DataLoader(val_segmented_data, shuffle = True, batch_size = 84, num_workers = 3)\n",
    "\n",
    "#visualization\n",
    "train_loader_vis = torch.utils.data.DataLoader(train_data, shuffle = True, batch_size = 1, num_workers = 3)\n",
    "\n",
    "print (\"Number of batches train set:{}\".format(len(train_loader)))\n",
    "print (\"Number of batches test set:{}\".format(len(test_loader)))\n",
    "print (\"Number of batches validation set:{}\".format(len(val_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (convolution_layer_1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      "  (batchNormalization1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchNormalization2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchNormalization3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (maxPooling_layer): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (maxPooling_layer1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (convolution_layer_2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (convolution_layer_3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (convolution_layer_4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (convolution_layer_5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (dropout): Dropout(p=0.25)\n",
      "  (fully_connected_layer): Linear(in_features=2304, out_features=1024, bias=True)\n",
      "  (fully_connected_layer1): Linear(in_features=1024, out_features=38, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.convolution_layer_1 = torch.nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size = 3)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.batchNormalization1 = torch.nn.BatchNorm2d(16)\n",
    "        self.batchNormalization2 = torch.nn.BatchNorm2d(32)\n",
    "        self.batchNormalization3 = torch.nn.BatchNorm2d(64)\n",
    "        self.maxPooling_layer = torch.nn.MaxPool2d(kernel_size = 3)\n",
    "        self.maxPooling_layer1 = torch.nn.MaxPool2d(kernel_size = 2)\n",
    "        self.convolution_layer_2 = torch.nn.Conv2d(in_channels = 16 , out_channels = 32, kernel_size = 3)\n",
    "        self.convolution_layer_3 = torch.nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3)\n",
    "        self.convolution_layer_4 = torch.nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3)\n",
    "        self.convolution_layer_5 = torch.nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3)\n",
    "        self.dropout = torch.nn.Dropout(0.25)\n",
    "        self.fully_connected_layer = torch.nn.Linear(in_features = 2304, out_features = 1024)\n",
    "        self.fully_connected_layer1 = torch.nn.Linear(in_features = 1024, out_features = 38)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convolution_layer_1(x)\n",
    "        x = self.relu(x)\n",
    "        #x = self.batchNormalization1(x)\n",
    "        x = self.maxPooling_layer(x)\n",
    "        #x = self.dropout(x)\n",
    "        \n",
    "        x = self.convolution_layer_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchNormalization2(x)\n",
    "        x = self.maxPooling_layer1(x)\n",
    "        #x = self.dropout(x)\n",
    "        \n",
    "        x = self.convolution_layer_3(x)\n",
    "        x = self.relu(x)\n",
    "        #x = self.batchNormalization2(x)\n",
    "        x = self.maxPooling_layer1(x)\n",
    "        #x = self.dropout(x)\n",
    "        \n",
    "        x = self.convolution_layer_4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchNormalization3(x)\n",
    "        \n",
    "        x = self.convolution_layer_5(x)\n",
    "        x = self.relu(x)\n",
    "        #x = self.batchNormalization3(x)\n",
    "        x = self.maxPooling_layer1(x)\n",
    "        #x = self.dropout(x)\n",
    "        \n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fully_connected_layer(x)\n",
    "        x = self.fully_connected_layer1(x)\n",
    "        return torch.nn.functional.log_softmax(x,dim=1)\n",
    "      \n",
    "classification_model = ConvNet()\n",
    "#torch.cuda.empty_cache()\n",
    "#model2 = model2.cuda()\n",
    "print(classification_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classification_model(data_loader, model, criterion, optimizer, lr_decay, num_epochs,mode='train'):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        loss_sum = []; acc = 0\n",
    "        for i in data_loader:\n",
    "            input_img =i[0].cuda()\n",
    "            target = i[1].cuda() \n",
    "            if mode=='train':\n",
    "                optimizer.zero_grad()\n",
    "            predicted_output = model(input_img)\n",
    "            loss = criterion(predicted_output, target)\n",
    "            _,pred_out = torch.max(predicted_output,1)    \n",
    "            if mode=='train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            loss_sum.append(loss.item())\n",
    "            acc+=torch.sum(pred_out==target.data)\n",
    "        epoch_loss = sum(loss_sum)/len(loss_sum)\n",
    "        epoch_accuracy = acc.item()/len(train_data)\n",
    "        print (\"epoch:\"+str(epoch))\n",
    "        print (\"Loss:{:.6f}\".format(epoch_loss))\n",
    "        print (\"Accuracy:{:.6f}\".format(epoch_accuracy))\n",
    "        print (\" \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test models\n",
    "def test_model(model,data_loader):\n",
    "    model.eval()\n",
    "    accuracy_list=[]\n",
    "    with torch.no_grad():\n",
    "        predicted_output_concat = np.zeros([84])\n",
    "        target_concat = np.zeros([84])\n",
    "        for i in data_loader:\n",
    "            input_img=i[0].cuda()\n",
    "            #print (input_img[0].shape)\n",
    "            target=i[1]\n",
    "            target=target.cuda()\n",
    "            pred_output = model(input_img)\n",
    "            eq = (target.data == torch.exp(pred_output).max(1)[1])\n",
    "            pred_out = torch.exp(pred_output).max(1)[1]\n",
    "            accuracy_list.append(eq.type_as(torch.FloatTensor()).mean())\n",
    "            predicted_output_concat = np.concatenate((predicted_output_concat, pred_out.cpu().detach().numpy()), axis = 0)\n",
    "            target_concat = np.concatenate((target_concat, target.cpu().detach().numpy()), axis = 0)\n",
    "        print (\"Test accuracy:{:.6f}\".format(sum(accuracy_list)/len(accuracy_list)))\n",
    "        #print (target_concat)\n",
    "        #print (target_concat.shape)\n",
    "        #print (predicted_output_concat)\n",
    "        #print (predicted_output_concat.shape)\n",
    "    print (\"Macro F1 score:{}\".format(f1_score(target_concat, predicted_output_concat, average=\"macro\")))\n",
    "    print (\"Micro F1 score:{}\".format(f1_score(target_concat, predicted_output_concat, average=\"micro\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam and 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model accuracies on color datatset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "Loss:0.700417\n",
      "Accuracy:0.792785\n",
      " \n",
      "epoch:1\n",
      "Loss:0.679126\n",
      "Accuracy:0.795215\n",
      " \n",
      "epoch:2\n",
      "Loss:0.714455\n",
      "Accuracy:0.791477\n",
      " \n",
      "epoch:3\n",
      "Loss:0.630987\n",
      "Accuracy:0.807613\n",
      " \n",
      "epoch:4\n",
      "Loss:0.597447\n",
      "Accuracy:0.818454\n",
      " \n",
      "epoch:5\n",
      "Loss:0.572220\n",
      "Accuracy:0.828796\n",
      " \n",
      "epoch:6\n",
      "Loss:0.577957\n",
      "Accuracy:0.827923\n",
      " \n",
      "epoch:7\n",
      "Loss:0.574554\n",
      "Accuracy:0.826304\n",
      " \n",
      "epoch:8\n",
      "Loss:0.532445\n",
      "Accuracy:0.836895\n",
      " \n",
      "epoch:9\n",
      "Loss:0.533674\n",
      "Accuracy:0.840758\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "criterion = torch.nn.NLLLoss()\n",
    "classification_model = classification_model.cuda()\n",
    "optimizer = torch.optim.Adam(classification_model.parameters(), lr=0.001)\n",
    "#lr_schedule = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "lr_schedule = 0 \n",
    "train_classification_model(train_loader, classification_model, criterion, optimizer,lr_schedule,10,'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.857872\n",
      "Macro F1 score:0.8181735264432933\n",
      "Micro F1 score:0.860395584176633\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.863233\n",
      "Macro F1 score:0.82222569502911\n",
      "Micro F1 score:0.8650757228086278\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model accuracies on segmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "Loss:2.205165\n",
      "Accuracy:0.400037\n",
      " \n",
      "epoch:1\n",
      "Loss:1.593978\n",
      "Accuracy:0.542458\n",
      " \n",
      "epoch:2\n",
      "Loss:1.369356\n",
      "Accuracy:0.598903\n",
      " \n",
      "epoch:3\n",
      "Loss:1.266563\n",
      "Accuracy:0.628434\n",
      " \n",
      "epoch:4\n",
      "Loss:1.175051\n",
      "Accuracy:0.651112\n",
      " \n",
      "epoch:5\n",
      "Loss:1.071412\n",
      "Accuracy:0.680082\n",
      " \n",
      "epoch:6\n",
      "Loss:1.043254\n",
      "Accuracy:0.689241\n",
      " \n",
      "epoch:7\n",
      "Loss:0.976179\n",
      "Accuracy:0.706249\n",
      " \n",
      "epoch:8\n",
      "Loss:0.970745\n",
      "Accuracy:0.708865\n",
      " \n",
      "epoch:9\n",
      "Loss:0.919327\n",
      "Accuracy:0.724628\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "criterion = torch.nn.NLLLoss()\n",
    "classification_model = classification_model.cuda()\n",
    "optimizer = torch.optim.Adam(classification_model.parameters(), lr=0.001)\n",
    "#lr_schedule = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "lr_schedule = 0 \n",
    "train_classification_model(train_segmented_loader, classification_model, criterion, optimizer,lr_schedule,10,'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.786287\n",
      "Macro F1 score:0.7158673769954519\n",
      "Micro F1 score:0.7901518637827888\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, val_segmented_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.777770\n",
      "Macro F1 score:0.7085493876514524\n",
      "Micro F1 score:0.7808575803981623\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, test_segmented_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model accuracies on grayscale dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "Loss:2.502543\n",
      "Accuracy:0.320915\n",
      " \n",
      "epoch:1\n",
      "Loss:1.956925\n",
      "Accuracy:0.448196\n",
      " \n",
      "epoch:2\n",
      "Loss:1.768757\n",
      "Accuracy:0.498723\n",
      " \n",
      "epoch:3\n",
      "Loss:1.599823\n",
      "Accuracy:0.538720\n",
      " \n",
      "epoch:4\n",
      "Loss:1.479176\n",
      "Accuracy:0.573921\n",
      " \n",
      "epoch:5\n",
      "Loss:1.393083\n",
      "Accuracy:0.599215\n",
      " \n",
      "epoch:6\n",
      "Loss:1.350538\n",
      "Accuracy:0.607003\n",
      " \n",
      "epoch:7\n",
      "Loss:1.301595\n",
      "Accuracy:0.616223\n",
      " \n",
      "epoch:8\n",
      "Loss:1.236184\n",
      "Accuracy:0.634478\n",
      " \n",
      "epoch:9\n",
      "Loss:1.202500\n",
      "Accuracy:0.646439\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "criterion = torch.nn.NLLLoss()\n",
    "classification_model = classification_model.cuda()\n",
    "optimizer = torch.optim.Adam(classification_model.parameters(), lr=0.001)\n",
    "#lr_schedule = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "lr_schedule = 0 \n",
    "train_classification_model(train_grayscale_loader, classification_model, criterion, optimizer,lr_schedule,10,'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.629844\n",
      "Macro F1 score:0.5509806852717541\n",
      "Micro F1 score:0.6373045078196872\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, val_grayscale_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.632814\n",
      "Macro F1 score:0.5666033094094647\n",
      "Micro F1 score:0.6377543215542297\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, test_grayscale_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam and 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model accuracies on colored dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "Loss:2.310437\n",
      "Accuracy:0.380412\n",
      " \n",
      "epoch:1\n",
      "Loss:1.681469\n",
      "Accuracy:0.526634\n",
      " \n",
      "epoch:2\n",
      "Loss:1.426972\n",
      "Accuracy:0.585073\n",
      " \n",
      "epoch:3\n",
      "Loss:1.257546\n",
      "Accuracy:0.630677\n",
      " \n",
      "epoch:4\n",
      "Loss:1.137535\n",
      "Accuracy:0.664445\n",
      " \n",
      "epoch:5\n",
      "Loss:1.063591\n",
      "Accuracy:0.685627\n",
      " \n",
      "epoch:6\n",
      "Loss:0.999239\n",
      "Accuracy:0.702635\n",
      " \n",
      "epoch:7\n",
      "Loss:0.951057\n",
      "Accuracy:0.712105\n",
      " \n",
      "epoch:8\n",
      "Loss:0.905986\n",
      "Accuracy:0.725749\n",
      " \n",
      "epoch:9\n",
      "Loss:0.860122\n",
      "Accuracy:0.745686\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "criterion = torch.nn.NLLLoss()\n",
    "classification_model = classification_model.cuda()\n",
    "optimizer = torch.optim.Adam(classification_model.parameters(), lr=0.0001)\n",
    "#lr_schedule = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "lr_schedule = 0 \n",
    "train_classification_model(train_loader, classification_model, criterion, optimizer,lr_schedule,10,'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.817592\n",
      "Macro F1 score:0.7565591310175958\n",
      "Micro F1 score:0.8210671573137075\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.820783\n",
      "Macro F1 score:0.7590345739015966\n",
      "Micro F1 score:0.8230074957931773\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Accuracies on segmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "Loss:2.353862\n",
      "Accuracy:0.371130\n",
      " \n",
      "epoch:1\n",
      "Loss:1.731928\n",
      "Accuracy:0.503458\n",
      " \n",
      "epoch:2\n",
      "Loss:1.516334\n",
      "Accuracy:0.562083\n",
      " \n",
      "epoch:3\n",
      "Loss:1.394320\n",
      "Accuracy:0.588437\n",
      " \n",
      "epoch:4\n",
      "Loss:1.296873\n",
      "Accuracy:0.620397\n",
      " \n",
      "epoch:5\n",
      "Loss:1.230812\n",
      "Accuracy:0.634851\n",
      " \n",
      "epoch:6\n",
      "Loss:1.160845\n",
      "Accuracy:0.653604\n",
      " \n",
      "epoch:7\n",
      "Loss:1.098012\n",
      "Accuracy:0.671298\n",
      " \n",
      "epoch:8\n",
      "Loss:1.064501\n",
      "Accuracy:0.680020\n",
      " \n",
      "epoch:9\n",
      "Loss:1.010322\n",
      "Accuracy:0.692480\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "criterion = torch.nn.NLLLoss()\n",
    "classification_model = classification_model.cuda()\n",
    "optimizer = torch.optim.Adam(classification_model.parameters(), lr=0.0001)\n",
    "#lr_schedule = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "lr_schedule = 0 \n",
    "train_classification_model(train_segmented_loader, classification_model, criterion, optimizer,lr_schedule,10,'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.742304\n",
      "Macro F1 score:0.6573911921578117\n",
      "Micro F1 score:0.7471237919926369\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, val_segmented_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.735826\n",
      "Macro F1 score:0.6664958612540932\n",
      "Micro F1 score:0.7392036753445637\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, test_segmented_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Accuracies on grayscale dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "Loss:2.667838\n",
      "Accuracy:0.290075\n",
      " \n",
      "epoch:1\n",
      "Loss:2.153155\n",
      "Accuracy:0.406081\n",
      " \n",
      "epoch:2\n",
      "Loss:1.945169\n",
      "Accuracy:0.459473\n",
      " \n",
      "epoch:3\n",
      "Loss:1.813996\n",
      "Accuracy:0.488755\n",
      " \n",
      "epoch:4\n",
      "Loss:1.712136\n",
      "Accuracy:0.514174\n",
      " \n",
      "epoch:5\n",
      "Loss:1.633236\n",
      "Accuracy:0.537474\n",
      " \n",
      "epoch:6\n",
      "Loss:1.588259\n",
      "Accuracy:0.548626\n",
      " \n",
      "epoch:7\n",
      "Loss:1.529038\n",
      "Accuracy:0.561336\n",
      " \n",
      "epoch:8\n",
      "Loss:1.472828\n",
      "Accuracy:0.575229\n",
      " \n",
      "epoch:9\n",
      "Loss:1.434384\n",
      "Accuracy:0.587066\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "criterion = torch.nn.NLLLoss()\n",
    "classification_model = classification_model.cuda()\n",
    "optimizer = torch.optim.Adam(classification_model.parameters(), lr=0.0001)\n",
    "#lr_schedule = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "lr_schedule = 0 \n",
    "train_classification_model(train_grayscale_loader, classification_model, criterion, optimizer,lr_schedule,10,'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.646840\n",
      "Macro F1 score:0.5261404522202555\n",
      "Micro F1 score:0.6534038638454461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u1136693/env_dir/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, val_grayscale_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.632895\n",
      "Macro F1 score:0.514897820559482\n",
      "Micro F1 score:0.6376013461832645\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, test_grayscale_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD + Momentum and 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Accuracies on colored dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "Loss:2.898652\n",
      "Accuracy:0.254688\n",
      " \n",
      "epoch:1\n",
      "Loss:2.245950\n",
      "Accuracy:0.404648\n",
      " \n",
      "epoch:2\n",
      "Loss:1.915377\n",
      "Accuracy:0.474051\n",
      " \n",
      "epoch:3\n",
      "Loss:1.649906\n",
      "Accuracy:0.535107\n",
      " \n",
      "epoch:4\n",
      "Loss:1.481189\n",
      "Accuracy:0.579279\n",
      " \n",
      "epoch:5\n",
      "Loss:1.338982\n",
      "Accuracy:0.613918\n",
      " \n",
      "epoch:6\n",
      "Loss:1.202439\n",
      "Accuracy:0.645692\n",
      " \n",
      "epoch:7\n",
      "Loss:1.112736\n",
      "Accuracy:0.673665\n",
      " \n",
      "epoch:8\n",
      "Loss:1.040124\n",
      "Accuracy:0.690175\n",
      " \n",
      "epoch:9\n",
      "Loss:0.972480\n",
      "Accuracy:0.709364\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "criterion = torch.nn.NLLLoss()\n",
    "classification_model = classification_model.cuda()\n",
    "optimizer = torch.optim.SGD(classification_model.parameters(), lr=0.001, momentum=0.9)\n",
    "#lr_schedule = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "lr_schedule = 0 \n",
    "train_classification_model(train_loader, classification_model, criterion, optimizer,lr_schedule,10,'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.697187\n",
      "Macro F1 score:0.6122174186503123\n",
      "Micro F1 score:0.703081876724931\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.685488\n",
      "Macro F1 score:0.5907739018389954\n",
      "Micro F1 score:0.6893070215695273\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Accuracies on segemented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "Loss:2.921082\n",
      "Accuracy:0.249517\n",
      " \n",
      "epoch:1\n",
      "Loss:2.331580\n",
      "Accuracy:0.374058\n",
      " \n",
      "epoch:2\n",
      "Loss:2.059590\n",
      "Accuracy:0.430440\n",
      " \n",
      "epoch:3\n",
      "Loss:1.822367\n",
      "Accuracy:0.482338\n",
      " \n",
      "epoch:4\n",
      "Loss:1.637186\n",
      "Accuracy:0.529126\n",
      " \n",
      "epoch:5\n",
      "Loss:1.510697\n",
      "Accuracy:0.552427\n",
      " \n",
      "epoch:6\n",
      "Loss:1.419749\n",
      "Accuracy:0.581210\n",
      " \n",
      "epoch:7\n",
      "Loss:1.346216\n",
      "Accuracy:0.601209\n",
      " \n",
      "epoch:8\n",
      "Loss:1.260093\n",
      "Accuracy:0.625444\n",
      " \n",
      "epoch:9\n",
      "Loss:1.205662\n",
      "Accuracy:0.637655\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "criterion = torch.nn.NLLLoss()\n",
    "classification_model = classification_model.cuda()\n",
    "optimizer = torch.optim.SGD(classification_model.parameters(), lr=0.001, momentum=0.9)\n",
    "#lr_schedule = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "lr_schedule = 0 \n",
    "train_classification_model(train_segmented_loader, classification_model, criterion, optimizer,lr_schedule,10,'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.686455\n",
      "Macro F1 score:0.5667361039253492\n",
      "Micro F1 score:0.6930510814542108\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, val_segmented_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.681130\n",
      "Macro F1 score:0.5619488126329447\n",
      "Micro F1 score:0.6852986217457887\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, test_segmented_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Accuracies on grayscale dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "Loss:3.083138\n",
      "Accuracy:0.196125\n",
      " \n",
      "epoch:1\n",
      "Loss:2.602838\n",
      "Accuracy:0.300542\n",
      " \n",
      "epoch:2\n",
      "Loss:2.401055\n",
      "Accuracy:0.349262\n",
      " \n",
      "epoch:3\n",
      "Loss:2.216840\n",
      "Accuracy:0.386082\n",
      " \n",
      "epoch:4\n",
      "Loss:2.094850\n",
      "Accuracy:0.420223\n",
      " \n",
      "epoch:5\n",
      "Loss:2.016675\n",
      "Accuracy:0.442278\n",
      " \n",
      "epoch:6\n",
      "Loss:1.931963\n",
      "Accuracy:0.460096\n",
      " \n",
      "epoch:7\n",
      "Loss:1.875110\n",
      "Accuracy:0.470812\n",
      " \n",
      "epoch:8\n",
      "Loss:1.780551\n",
      "Accuracy:0.496106\n",
      " \n",
      "epoch:9\n",
      "Loss:1.738123\n",
      "Accuracy:0.510311\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "criterion = torch.nn.NLLLoss()\n",
    "classification_model = classification_model.cuda()\n",
    "optimizer = torch.optim.SGD(classification_model.parameters(), lr=0.001, momentum=0.9)\n",
    "#lr_schedule = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "lr_schedule = 0 \n",
    "train_classification_model(train_grayscale_loader, classification_model, criterion, optimizer,lr_schedule,10,'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.594961\n",
      "Macro F1 score:0.4294813730969562\n",
      "Micro F1 score:0.6025758969641214\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, val_grayscale_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.586466\n",
      "Macro F1 score:0.41904708351464925\n",
      "Micro F1 score:0.5918617102646474\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, test_grayscale_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD + Momentum and 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Accuracies on colored dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "Loss:3.536460\n",
      "Accuracy:0.110897\n",
      " \n",
      "epoch:1\n",
      "Loss:3.287258\n",
      "Accuracy:0.181796\n",
      " \n",
      "epoch:2\n",
      "Loss:3.030194\n",
      "Accuracy:0.230204\n",
      " \n",
      "epoch:3\n",
      "Loss:2.882275\n",
      "Accuracy:0.255000\n",
      " \n",
      "epoch:4\n",
      "Loss:2.777023\n",
      "Accuracy:0.271634\n",
      " \n",
      "epoch:5\n",
      "Loss:2.673358\n",
      "Accuracy:0.305651\n",
      " \n",
      "epoch:6\n",
      "Loss:2.571978\n",
      "Accuracy:0.331630\n",
      " \n",
      "epoch:7\n",
      "Loss:2.479977\n",
      "Accuracy:0.352751\n",
      " \n",
      "epoch:8\n",
      "Loss:2.390455\n",
      "Accuracy:0.372500\n",
      " \n",
      "epoch:9\n",
      "Loss:2.318445\n",
      "Accuracy:0.383901\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "criterion = torch.nn.NLLLoss()\n",
    "classification_model = classification_model.cuda()\n",
    "optimizer = torch.optim.SGD(classification_model.parameters(), lr=0.0001, momentum=0.9)\n",
    "#lr_schedule = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "lr_schedule = 0 \n",
    "train_classification_model(train_loader, classification_model, criterion, optimizer,lr_schedule,10,'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.375875\n",
      "Macro F1 score:0.17184798723391512\n",
      "Micro F1 score:0.3882244710211591\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.387775\n",
      "Macro F1 score:0.175344178285155\n",
      "Micro F1 score:0.3959002600581306\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Accuracies on segmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "Loss:3.496635\n",
      "Accuracy:0.117687\n",
      " \n",
      "epoch:1\n",
      "Loss:3.225508\n",
      "Accuracy:0.205096\n",
      " \n",
      "epoch:2\n",
      "Loss:3.015645\n",
      "Accuracy:0.233256\n",
      " \n",
      "epoch:3\n",
      "Loss:2.875058\n",
      "Accuracy:0.248770\n",
      " \n",
      "epoch:4\n",
      "Loss:2.779653\n",
      "Accuracy:0.272506\n",
      " \n",
      "epoch:5\n",
      "Loss:2.689734\n",
      "Accuracy:0.294312\n",
      " \n",
      "epoch:6\n",
      "Loss:2.605827\n",
      "Accuracy:0.314809\n",
      " \n",
      "epoch:7\n",
      "Loss:2.545349\n",
      "Accuracy:0.323718\n",
      " \n",
      "epoch:8\n",
      "Loss:2.494230\n",
      "Accuracy:0.334808\n",
      " \n",
      "epoch:9\n",
      "Loss:2.444059\n",
      "Accuracy:0.347455\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "criterion = torch.nn.NLLLoss()\n",
    "classification_model = classification_model.cuda()\n",
    "optimizer = torch.optim.SGD(classification_model.parameters(), lr=0.0001, momentum=0.9)\n",
    "#lr_schedule = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "lr_schedule = 0 \n",
    "train_classification_model(train_segmented_loader, classification_model, criterion, optimizer,lr_schedule,10,'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.359485\n",
      "Macro F1 score:0.1454121579791586\n",
      "Micro F1 score:0.3722963644730787\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, val_segmented_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.365951\n",
      "Macro F1 score:0.14705508710769\n",
      "Micro F1 score:0.37457886676875957\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, test_segmented_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Accuracies on grayscale dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "Loss:3.527983\n",
      "Accuracy:0.112205\n",
      " \n",
      "epoch:1\n",
      "Loss:3.280571\n",
      "Accuracy:0.173696\n",
      " \n",
      "epoch:2\n",
      "Loss:3.125444\n",
      "Accuracy:0.185160\n",
      " \n",
      "epoch:3\n",
      "Loss:3.048638\n",
      "Accuracy:0.199738\n",
      " \n",
      "epoch:4\n",
      "Loss:2.987383\n",
      "Accuracy:0.213632\n",
      " \n",
      "epoch:5\n",
      "Loss:2.916442\n",
      "Accuracy:0.231325\n",
      " \n",
      "epoch:6\n",
      "Loss:2.855892\n",
      "Accuracy:0.250265\n",
      " \n",
      "epoch:7\n",
      "Loss:2.793217\n",
      "Accuracy:0.262414\n",
      " \n",
      "epoch:8\n",
      "Loss:2.723363\n",
      "Accuracy:0.278861\n",
      " \n",
      "epoch:9\n",
      "Loss:2.674181\n",
      "Accuracy:0.287396\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "criterion = torch.nn.NLLLoss()\n",
    "classification_model = classification_model.cuda()\n",
    "optimizer = torch.optim.SGD(classification_model.parameters(), lr=0.0001, momentum=0.9)\n",
    "#lr_schedule = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "lr_schedule = 0 \n",
    "train_classification_model(train_grayscale_loader, classification_model, criterion, optimizer,lr_schedule,10,'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.283818\n",
      "Macro F1 score:0.10941926255540392\n",
      "Micro F1 score:0.29737810487580496\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, val_grayscale_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.294218\n",
      "Macro F1 score:0.1117551411779298\n",
      "Micro F1 score:0.30319718525317424\n"
     ]
    }
   ],
   "source": [
    "test_model(classification_model, test_grayscale_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dir",
   "language": "python",
   "name": "env_dir"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
